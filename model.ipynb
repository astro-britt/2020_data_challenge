{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\britt\\miniconda3\\envs\\bobcat_env_36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\britt\\miniconda3\\envs\\bobcat_env_36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\britt\\miniconda3\\envs\\bobcat_env_36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\britt\\miniconda3\\envs\\bobcat_env_36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\britt\\miniconda3\\envs\\bobcat_env_36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\britt\\miniconda3\\envs\\bobcat_env_36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\britt\\miniconda3\\envs\\bobcat_env_36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\britt\\miniconda3\\envs\\bobcat_env_36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\britt\\miniconda3\\envs\\bobcat_env_36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\britt\\miniconda3\\envs\\bobcat_env_36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\britt\\miniconda3\\envs\\bobcat_env_36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\britt\\miniconda3\\envs\\bobcat_env_36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\britt\\miniconda3\\envs\\bobcat_env_36\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3072: DtypeWarning: Columns (61,63) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "test_boxes = pd.read_csv('tbl_test_boxes.csv')\n",
    "train_boxes = pd.read_csv('tbl_train_boxes.csv')\n",
    "unboxer_attributes = pd.read_csv('tbl_unboxer_attributes.csv')\n",
    "unboxer_keywords = pd.read_csv('tbl_unboxer_keywords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vendor_keywords = pd.read_csv('tbl_vendor_keywords.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_nans_by_column(df):\n",
    "    for col in df.columns:\n",
    "        n_nans = len(df[df[col].isna()==True])\n",
    "        print('{}: {} nans'.format(col, n_nans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple enough\n",
    "#train_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unboxer_id: 0 nans\n",
      "vendor_id: 0 nans\n",
      "target_bobcat: 0 nans\n"
     ]
    }
   ],
   "source": [
    "check_for_nans_by_column(train_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same\n",
    "#test_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unboxer_id: 0 nans\n",
      "vendor_id: 0 nans\n",
      "target_bobcat: 116837 nans\n"
     ]
    }
   ],
   "source": [
    "check_for_nans_by_column(test_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will need to decide how to handle nans\n",
    "# beartrap variables are categorical - will need to one-hot-encode\n",
    "\n",
    "# it seems that skimask features can be 0, 1, or -1\n",
    "# I assume these are meant to mean null, positive, or negative\n",
    "# so then it would make sense to replace nulls with 0s\n",
    "# then I could one hot encode \n",
    "\n",
    "#unboxer_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unboxer_id: 0 nans\n",
      "feature_skimask1: 6004 nans\n",
      "feature_skimask2: 5199 nans\n",
      "feature_skimask3: 5090 nans\n",
      "feature_skimask4: 5971 nans\n",
      "feature_skimask5: 4770 nans\n",
      "feature_skimask6: 6109 nans\n",
      "feature_skimask7: 4786 nans\n",
      "feature_skimask8: 6088 nans\n",
      "feature_skimask9: 5761 nans\n",
      "feature_skimask10: 4667 nans\n",
      "feature_skimask11: 6239 nans\n",
      "feature_skimask12: 5443 nans\n",
      "feature_skimask13: 5622 nans\n",
      "feature_skimask14: 5525 nans\n",
      "feature_skimask15: 6207 nans\n",
      "feature_skimask16: 5788 nans\n",
      "feature_skimask17: 4805 nans\n",
      "feature_skimask18: 5486 nans\n",
      "feature_skimask19: 4828 nans\n",
      "feature_skimask20: 4979 nans\n",
      "feature_skimask21: 5575 nans\n",
      "feature_skimask22: 5830 nans\n",
      "feature_skimask23: 4957 nans\n",
      "feature_skimask24: 6297 nans\n",
      "feature_skimask25: 5706 nans\n",
      "feature_skimask26: 5132 nans\n",
      "feature_skimask27: 5058 nans\n",
      "feature_skimask28: 6191 nans\n",
      "feature_skimask29: 5305 nans\n",
      "feature_skimask30: 5414 nans\n",
      "feature_skimask31: 5866 nans\n",
      "feature_skimask32: 5455 nans\n",
      "feature_skimask33: 5607 nans\n",
      "feature_skimask34: 5278 nans\n",
      "feature_skimask35: 4917 nans\n",
      "feature_skimask36: 5021 nans\n",
      "feature_skimask37: 5495 nans\n",
      "feature_skimask38: 4733 nans\n",
      "feature_skimask39: 4710 nans\n",
      "feature_skimask40: 5900 nans\n",
      "feature_skimask41: 4990 nans\n",
      "feature_skimask42: 4932 nans\n",
      "feature_skimask43: 6314 nans\n",
      "feature_skimask44: 6344 nans\n",
      "feature_skimask45: 5679 nans\n",
      "feature_skimask46: 5165 nans\n",
      "feature_skimask47: 6164 nans\n",
      "feature_skimask48: 5360 nans\n",
      "feature_skimask49: 6058 nans\n",
      "feature_skimask50: 4898 nans\n",
      "feature_skimask51: 6029 nans\n",
      "feature_skimask52: 4753 nans\n",
      "feature_skimask53: 6130 nans\n",
      "feature_skimask54: 4851 nans\n",
      "feature_skimask55: 5398 nans\n",
      "feature_skimask56: 4879 nans\n",
      "feature_skimask57: 5552 nans\n",
      "feature_skimask58: 5245 nans\n",
      "feature_skimask59: 5938 nans\n",
      "feature_skimask60: 6275 nans\n",
      "feature_beartrap1: 6338 nans\n",
      "feature_beartrap2: 224 nans\n",
      "feature_beartrap3: 66 nans\n",
      "feature_beartrap4: 0 nans\n",
      "feature_beartrap5: 925 nans\n"
     ]
    }
   ],
   "source": [
    "check_for_nans_by_column(unboxer_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical - simple enough\n",
    "# need to switch enabled to 0s and 1s\n",
    "#unboxer_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unboxer_id: 0 nans\n",
      "keyword_id: 0 nans\n",
      "enabled: 0 nans\n"
     ]
    }
   ],
   "source": [
    "check_for_nans_by_column(unboxer_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again, need to deal with nans - how many rows are all nan?\n",
    "# need to switch enabled to 0s and 1s\n",
    "# vendor_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vendor_id: 614822 nans\n",
      "keyword_id: 614822 nans\n",
      "enabled: 614822 nans\n"
     ]
    }
   ],
   "source": [
    "# so it seems that we can just discard any rows with a nan\n",
    "check_for_nans_by_column(vendor_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data \n",
    "Let's move backwards on this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with nans\n",
    "vendor_keywords = vendor_keywords.dropna()\n",
    "\n",
    "# replace -1s with 0s\n",
    "vendor_keywords = vendor_keywords.replace(-1, 0)\n",
    "\n",
    "# SUMBSAMPLE FOR NOW BECAUSE IM IMPATIENT: TODO: FIX THIS LATER\n",
    "#print(len(vendor_keywords))\n",
    "#vendor_keywords = vendor_keywords.sample(frac=0.01, random_state=42)\n",
    "#print(len(vendor_keywords))\n",
    "\n",
    "# need to one hot encode\n",
    "vendor_keywords['keyword_id'] = vendor_keywords['keyword_id'].astype('category')\n",
    "vendor_keywords = pd.get_dummies(vendor_keywords)\n",
    "cols_to_multiply = vendor_keywords.columns.to_list()[2:]\n",
    "print('multiplying...')\n",
    "vendor_keywords[cols_to_multiply] = vendor_keywords[cols_to_multiply].multiply(vendor_keywords['enabled'], axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vendor_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to aggregate by user_id\n",
    "print('aggregating...')\n",
    "vendor_keywords_agg = vendor_keywords.groupby('vendor_id').sum()\n",
    "vendor_keywords_agg = vendor_keywords_agg.drop(columns='enabled')\n",
    "vendor_keywords_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace -1s with 0s\n",
    "unboxer_keywords = unboxer_keywords.replace(-1, 0)\n",
    "\n",
    "# SUMBSAMPLE FOR NOW BECAUSE IM IMPATIENT: TODO: FIX THIS LATER\n",
    "#print(len(unboxer_keywords))\n",
    "#unboxer_keywords = unboxer_keywords.sample(frac=0.5, random_state=42)\n",
    "#print(len(unboxer_keywords))\n",
    "\n",
    "# need to one hot encode\n",
    "unboxer_keywords['keyword_id'] = unboxer_keywords['keyword_id'].astype('category')\n",
    "unboxer_keywords = pd.get_dummies(unboxer_keywords)\n",
    "cols_to_multiply = unboxer_keywords.columns.to_list()[2:]\n",
    "print('multiplying...')\n",
    "unboxer_keywords[cols_to_multiply] = unboxer_keywords[cols_to_multiply].multiply(unboxer_keywords['enabled'], axis='index')\n",
    "\n",
    "# need to aggregate by user_id\n",
    "print('aggregating...')\n",
    "unboxer_keywords_agg = unboxer_keywords.groupby('unboxer_id').sum()\n",
    "unboxer_keywords_agg = unboxer_keywords_agg.drop(columns='enabled')\n",
    "unboxer_keywords_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### replace nans with 0s \n",
    "unboxer_attributes = unboxer_attributes.fillna(0)\n",
    "\n",
    "# now everything can be one-hot-encoded\n",
    "# going to have so many columns... can definitely remove correlated/insignificant ones later if needed\n",
    "\n",
    "# first we need to make all of the columns categorical\n",
    "def make_all_cols_categorical(df):\n",
    "    for col in df.columns:\n",
    "        if col != 'unboxer_id':\n",
    "            df[col] = df[col].astype('category')\n",
    "    return(df)\n",
    "unboxer_attributes = make_all_cols_categorical(unboxer_attributes)\n",
    "# do the one hot encoding\n",
    "unboxer_attributes = pd.get_dummies(unboxer_attributes)\n",
    "\n",
    "unboxer_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_boxes and train_boxes are fine for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the tables to make one big dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let me subsample to make things run quicky for now\n",
    "# when I have a model I can remove insignificant/correlated features and use the whole dataset\n",
    "train_boxes_subsampled = train_boxes.sample(frac=.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_boxes_subsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('So I will expect to see maximum {} rows throughout this whole merging process'.format(len(train_boxes_subsampled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(unboxer_keywords_agg), len(train_boxes_subsampled), len(unboxer_keywords_agg)*len(train_boxes_subsampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the unboxer keywords\n",
    "\n",
    "# left.merge(right, on='user_id', how='left')\n",
    "train_full_data = pd.merge(train_boxes_subsampled, unboxer_keywords_agg, on='unboxer_id', how='left')\n",
    "# TODO: remove this later\n",
    "# since I sampled the unboxer_keywords df, there are some unboxers which are not represented. \n",
    "# Drop their rows for now\n",
    "# train_full_data = train_full_data.dropna()\n",
    "# train_full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the vendor keywords \n",
    "train_full_data = pd.merge(train_full_data, vendor_keywords_agg, on='vendor_id', how='left')\n",
    "#train_full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join unboxer attributes\n",
    "train_full_data = pd.merge(train_full_data, unboxer_attributes, on='unboxer_id', how='left')\n",
    "# train_full_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert dataframe to features and targets arrays, split train and test, deal with unbalanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to deal with unbalanced classes- we want an equal number of bobcats and not bobcats\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = train_full_data[train_full_data.target_bobcat==0]\n",
    "df_minority = train_full_data[train_full_data.target_bobcat==1]\n",
    " \n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,                  # sample with replacement\n",
    "                                 n_samples=len(df_majority),    # to match majority class\n",
    "                                 random_state=42)               # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "train_full_data = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "# remove the unneccessary columns\n",
    "train_full_data = train_full_data.drop(columns=['unboxer_id', 'vendor_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into train and test sets\n",
    "train, test = train_test_split(train_full_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split features from targets, convert to array\n",
    "x_train = train.drop(columns='target_bobcat').values\n",
    "y_train = train['target_bobcat'].values\n",
    "x_test = test.drop(columns='target_bobcat').values\n",
    "y_test = test['target_bobcat'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check- no nans in input?\n",
    "def check_any_nans(df):\n",
    "    return df.isnull().any() \n",
    "\n",
    "for df in [train, test]:\n",
    "    print(check_any_nans(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and train a model \n",
    "If 7.5% of boxes contain a bobcat, then if my model is choosing randomly it would get accuracy=0.075. This is the number to beat for a performant model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# start with something very simple\n",
    "model.add(Dense(500, input_dim=x_train.shape[1], activation='relu', kernel_initializer='he_uniform'))  # input layer\n",
    "model.add(Dense(200, activation='relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid')) # sigmoid to choose 1 or 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3, batch_size=32)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
